{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Target Files\n",
    "\n",
    "This notebook loads DHS survey data, computes aggregated mean statistics for each cluster, and stores the data in a `targets.json` file for the specified AOI. Geographic plots are also generated to visualize the cluster-level statistics, and variograms of the survey metrics are produced.\n",
    "\n",
    "This notebook makes use of lower-level code in `project_utils`.\n",
    "\n",
    "\n",
    "## Input\n",
    "\n",
    "DHS data is used as the basis for creating partition maps for each country, based on the location of clusters.\n",
    "\n",
    "\n",
    "\n",
    "<pre style=\"font-family: monospace;\">\n",
    "./GIS-Image-Stack-Processing\n",
    "    /DHS\n",
    "        /County specific folders containing DHS files\n",
    "</pre>\n",
    "\n",
    "## Output\n",
    "<pre style=\"font-family: monospace;\">\n",
    "./GIS-Image-Stack-Processing\n",
    "    /AOI/\n",
    "        PK/\n",
    "            Targets/\n",
    "                <span style=\"color: blue;\">targets.json</span> \n",
    "        TD/\n",
    "             Targets/\n",
    "                <span style=\"color: blue;\">targets.json</span> \n",
    "\n",
    "</pre>\n",
    "\n",
    "## Required Configurations\n",
    "\n",
    "<pre style=\"font-family: monospace;\">\n",
    "<span style=\"color: blue;\">country_code  = 'PK'</span>      # Set the country code\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# REQUIRED CONFIGURATIONS HERE\n",
    "#-------------------------------------------------\n",
    "country_code  = 'PK'      # Set the country code\n",
    "#-------------------------------------------------\n",
    "\n",
    "CREATE_TARGETS = False\n",
    "CREATE_DHS_MAPS = True\n",
    "\n",
    "cluster_colors = ['blue', 'red', 'green', 'purple', 'orange', 'pink', 'olive', 'teal', 'navy']\n",
    "symbol_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"a6c301c5-5da4-4b64-8d69-087d39fb5ee1\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"a6c301c5-5da4-4b64-8d69-087d39fb5ee1\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.2.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"a6c301c5-5da4-4b64-8d69-087d39fb5ee1\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"a6c301c5-5da4-4b64-8d69-087d39fb5ee1\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"a6c301c5-5da4-4b64-8d69-087d39fb5ee1\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tempfile\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyreadstat\n",
    "\n",
    "from bokeh.palettes import Viridis256\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from selenium import webdriver\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# from skgstat import Variogram\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./GIS-Image-Stack-Processing') \n",
    "\n",
    "cache_dir = 'project_utils/__pycache__'\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "\n",
    "# Import module that contains several convenience functions (e.g., gdal wrappers)\n",
    "from project_utils import *\n",
    "from project_utils.aoi_configurations import aoi_configurations\n",
    "from project_utils.plot_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Results Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerated list of DHS target values\n",
    "@dataclass(frozen=True)\n",
    "class TargetType(Enum):\n",
    "    FRACTION_DPT3_VACCINATED  = \"fraction_dpt3_vaccinated\"\n",
    "    FRACTION_WITH_ELECTRICITY = \"fraction_with_electricity\"\n",
    "    FRACTION_WITH_FRESH_WATER = \"fraction_with_fresh_water\"\n",
    "    MEAN_WEALTH_INDEX         = \"mean_wealth_index\"\n",
    "    FRACTION_WITH_RADIO       = 'fraction_with_radio'\n",
    "    FRACTION_WITH_TV          = \"fraction_with_tv\"\n",
    "\n",
    "# Result configurations\n",
    "@dataclass(frozen=True)\n",
    "class ResultsConfig:\n",
    "    COMPUTE_GEOSPATIAL: bool = False\n",
    "    PLOT_GEOSPATIAL_DIR: str = './Plots_Geospatial'\n",
    "    PLOT_VARIOGRAM_DIR:  str = './Plots_Variograms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIS_ROOT = './GIS-Image-Stack-Processing'\n",
    "PRT_ROOT = './GIS-Image-Stack-Processing/AOI/Partitions'\n",
    "\n",
    "target_json_path = os.path.join(GIS_ROOT, f'AOI/{country_code}/Targets/targets.json')\n",
    "\n",
    "results_config = ResultsConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DHS Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shapefile_path = os.path.join(GIS_ROOT, aoi_configurations[country_code]['shapefile'])\n",
    "recode_hr_path = os.path.join(GIS_ROOT, aoi_configurations[country_code]['recode_hr'])\n",
    "recode_kr_path = os.path.join(GIS_ROOT, aoi_configurations[country_code]['recode_kr'])\n",
    "\n",
    "# DHS Column Headings\n",
    "dhs_cluster_field  = 'DHSCLUST'\n",
    "dhs_lat_field      = 'LATNUM'\n",
    "dhs_lon_field      = 'LONGNUM'\n",
    "\n",
    "# Map Heading to new names\n",
    "cluster_id   = 'cluster_id'\n",
    "cluster_lat  = 'lat'\n",
    "cluster_lon  = 'lon'\n",
    "\n",
    "# The following mappings are used to rename DHS column headings to more meaningful names\n",
    "cluster_column_mapping = {\n",
    "    dhs_cluster_field: cluster_id,\n",
    "    dhs_lat_field: cluster_lat,\n",
    "    dhs_lon_field: cluster_lon\n",
    "}\n",
    "\n",
    "# DHS Household recode column name mapping\n",
    "hr_column_mapping = {\n",
    "    'HV001': cluster_id,\n",
    "    'HV201': 'water_access',\n",
    "    'HV206': 'electricity_access',\n",
    "    'HV209': 'radio_access',\n",
    "    'HV210': 'tv_access',\n",
    "    'HV270': 'wealth_index'\n",
    "}\n",
    "\n",
    "# DHS Child recode column name mapping\n",
    "kr_column_mapping = {\n",
    "    'V001': cluster_id,\n",
    "    'H7': 'dpt1',\n",
    "    'H8': 'dpt2',\n",
    "    'H9': 'dpt3'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract DHS Cluster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billk/dev/BMGF/Deep-Learning-Global-Health-Analytics/./GIS-Image-Stack-Processing/project_utils/gist_utils.py:536: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_data[cluster_field] = cluster_data[cluster_field].astype(float).astype(int)\n"
     ]
    }
   ],
   "source": [
    "cluster_df, erroneous_cluster_ids = extract_cluster_data(shapefile_path, \n",
    "                                                         dhs_cluster_field, \n",
    "                                                         dhs_lat_field, \n",
    "                                                         dhs_lon_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(erroneous_cluster_ids)\n",
    "#-------------------------------------------------------------\n",
    "# Handle special case for MR. The geospatial data for clutser \n",
    "# 714 has numerical issues, for now just remove that record.\n",
    "#-------------------------------------------------------------\n",
    "if country_code == 'MR':\n",
    "    erroneous_cluster_ids.append(714)\n",
    "print(erroneous_cluster_ids)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster_id        lat        lon\n",
      "0           1  31.980907  35.910510\n",
      "1           2  31.975399  35.896461\n",
      "2           3  31.957432  35.901973\n",
      "3           4  31.973120  35.918679\n",
      "4           5  31.956353  35.924025\n",
      "970\n"
     ]
    }
   ],
   "source": [
    "# Use the mapping to select and rename columns\n",
    "cluster_df = cluster_df[list(cluster_column_mapping.keys())].rename(columns=cluster_column_mapping)\n",
    "\n",
    "print(cluster_df.head())\n",
    "print(cluster_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970\n",
      "970\n"
     ]
    }
   ],
   "source": [
    "print(len(cluster_df))\n",
    "last_cluster_id = cluster_df[cluster_id].iloc[-1]\n",
    "print(last_cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cluster_id        lat        lon\n",
      "0            1  31.980907  35.910510\n",
      "1            2  31.975399  35.896461\n",
      "2            3  31.957432  35.901973\n",
      "3            4  31.973120  35.918679\n",
      "4            5  31.956353  35.924025\n",
      "..         ...        ...        ...\n",
      "965        966  29.913095  35.439521\n",
      "966        967  29.973032  35.462851\n",
      "967        968  29.647948  35.502176\n",
      "968        969  29.674336  35.582671\n",
      "969        970  29.560721  35.563241\n",
      "\n",
      "[970 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "cluster_df[cluster_id] = cluster_df[cluster_id].astype(str)  # Convert to string\n",
    "print(cluster_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DHS Household Recode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HV001  HV201  HV206  HV209  HV210  HV270\n",
      "0    1.0   11.0    NaN    1.0    NaN    4.0\n",
      "1    1.0   11.0    NaN    1.0    NaN    3.0\n",
      "2    1.0   71.0    NaN    1.0    NaN    5.0\n",
      "3    1.0   71.0    NaN    1.0    NaN    5.0\n",
      "4    1.0   71.0    NaN    1.0    NaN    4.0\n"
     ]
    }
   ],
   "source": [
    "selected_columns = list(hr_column_mapping.keys())\n",
    "\n",
    "# Load the selected columns\n",
    "hr_df, meta = pyreadstat.read_sav(recode_hr_path, usecols=selected_columns)\n",
    "\n",
    "# Check if DataFrame is loaded\n",
    "if hr_df is not None:\n",
    "    print(hr_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"HV201\": {\n",
      "        \"10.0\": \"PIPED WATER\",\n",
      "        \"11.0\": \"Piped into dwelling\",\n",
      "        \"12.0\": \"Piped to yard/plot\",\n",
      "        \"13.0\": \"Piped to neighbor\",\n",
      "        \"14.0\": \"Public tap/standpipe\",\n",
      "        \"20.0\": \"TUBE WELL WATER\",\n",
      "        \"21.0\": \"Tube well or borehole\",\n",
      "        \"30.0\": \"DUG WELL (OPEN/PROTECTED)\",\n",
      "        \"31.0\": \"Protected well\",\n",
      "        \"32.0\": \"Unprotected well\",\n",
      "        \"40.0\": \"SURFACE FROM SPRING\",\n",
      "        \"41.0\": \"Protected spring\",\n",
      "        \"42.0\": \"Unprotected spring\",\n",
      "        \"43.0\": \"River/dam/lake/ponds/stream/canal/irrigation channel\",\n",
      "        \"51.0\": \"Rainwater\",\n",
      "        \"61.0\": \"Tanker truck\",\n",
      "        \"62.0\": \"Cart with small tank\",\n",
      "        \"71.0\": \"Bottled water\",\n",
      "        \"96.0\": \"Other\"\n",
      "    },\n",
      "    \"HV206\": {\n",
      "        \"0.0\": \"No\",\n",
      "        \"1.0\": \"Yes\"\n",
      "    },\n",
      "    \"HV209\": {\n",
      "        \"0.0\": \"No\",\n",
      "        \"1.0\": \"Yes\"\n",
      "    },\n",
      "    \"HV210\": {\n",
      "        \"0.0\": \"No\",\n",
      "        \"1.0\": \"Yes\"\n",
      "    },\n",
      "    \"HV270\": {\n",
      "        \"1.0\": \"Poorest\",\n",
      "        \"2.0\": \"Poorer\",\n",
      "        \"3.0\": \"Middle\",\n",
      "        \"4.0\": \"Richer\",\n",
      "        \"5.0\": \"Richest\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(meta.variable_value_labels, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_access_labels = {\n",
    "    10.0: 'PIPED WATER',\n",
    "    11.0: 'Piped into dwelling',\n",
    "    12.0: 'Piped to yard/plot',\n",
    "    13.0: \"Piped to neighbor\",\n",
    "    14.0: 'Public tap/standpipe',\n",
    "    20.0: 'TUBE WELL WATER',\n",
    "    21.0: 'Tube well or borehole',\n",
    "    31.0: 'Protected well',\n",
    "    41.0: 'Protected spring',\n",
    "    51.0: 'Rainwater',\n",
    "    71.0: 'Bottled water',\n",
    "}\n",
    "\n",
    "electricity_access_labels = {\n",
    "    0.0: 'No',\n",
    "    1.0: 'Yes'\n",
    "}\n",
    "\n",
    "\n",
    "radio_access_labels = {\n",
    "    0.0: 'No',\n",
    "    1.0: 'Yes'\n",
    "}\n",
    "\n",
    "\n",
    "tv_access_labels = {\n",
    "    0.0: 'No',\n",
    "    1.0: 'Yes'\n",
    "}\n",
    "\n",
    "wealth_index_labels = {\n",
    "    1.0: 'Poorest',\n",
    "    2.0: 'Poorer',\n",
    "    3.0: 'Middle',\n",
    "    4.0: 'Richer',\n",
    "    5.0: 'Richest'\n",
    "}\n",
    "\n",
    "hr_df['HV201'] = hr_df['HV201'].map(water_access_labels)\n",
    "hr_df['HV206'] = hr_df['HV206'].map(electricity_access_labels)\n",
    "hr_df['HV209'] = hr_df['HV209'].map(radio_access_labels)  \n",
    "hr_df['HV210'] = hr_df['HV210'].map(tv_access_labels) \n",
    "hr_df['HV270'] = hr_df['HV270'].map(wealth_index_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster_id         water_access electricity_access radio_access tv_access  \\\n",
      "0         1.0  Piped into dwelling                NaN          Yes       NaN   \n",
      "1         1.0  Piped into dwelling                NaN          Yes       NaN   \n",
      "2         1.0        Bottled water                NaN          Yes       NaN   \n",
      "3         1.0        Bottled water                NaN          Yes       NaN   \n",
      "4         1.0        Bottled water                NaN          Yes       NaN   \n",
      "\n",
      "  wealth_index  \n",
      "0       Richer  \n",
      "1       Middle  \n",
      "2      Richest  \n",
      "3      Richest  \n",
      "4       Richer  \n"
     ]
    }
   ],
   "source": [
    "hr_df.rename(columns=hr_column_mapping, inplace=True)\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(hr_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values across relevant columns (upfront)\n",
    "relevant_columns = ['electricity_access', 'water_access', 'radio_access', 'tv_access', 'wealth_index']\n",
    "hr_df = hr_df.dropna(subset=relevant_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for missing or invalid values in 'cluster_id'\n",
    "print(hr_df['cluster_id'].isnull().sum())  # Print number of missing values\n",
    "\n",
    "# If there are non-numeric values, print those rows to inspect\n",
    "print(hr_df[~hr_df['cluster_id'].apply(lambda x: str(x).replace('.', '', 1).isdigit())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cluster_id, water_access, electricity_access, radio_access, tv_access, wealth_index]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Convert 'cluster_id' to integer (handling potential float values)\n",
    "hr_df['cluster_id'] = hr_df['cluster_id'].astype(float).astype(int)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(hr_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(hr_df.index.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Ensure columns have consistent labels and expected categories\n",
    "print(hr_df['water_access'].unique()) \n",
    "print(hr_df['electricity_access'].unique()) \n",
    "print(hr_df['wealth_index'].unique()) \n",
    "print(hr_df['radio_access'].unique())\n",
    "print(hr_df['tv_access'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_clusters(df, cluster_ids_to_remove, cluster_id_column='cluster_id'):\n",
    "    \n",
    "    # Ensure the cluster ID is treated as a column, whether it is currently an index or not\n",
    "    if cluster_id_column in df.index.names:\n",
    "        df = df.reset_index()\n",
    "\n",
    "    # Filter out rows where the cluster ID is in the list to remove\n",
    "    df_filtered = df[~df[cluster_id_column].isin(cluster_ids_to_remove)]\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DHS: Fresh Water Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cluster_id, fraction_with_fresh_water]\n",
      "Index: []\n",
      "0\n",
      "0\n",
      "Average Fraction of Households with Fresh Water Access across all clusters: nan%\n"
     ]
    }
   ],
   "source": [
    "# Define the categories that represent fresh water sources\n",
    "fresh_water_categories = {\n",
    "    'PIPED WATER',\n",
    "    'Piped into dwelling',\n",
    "    'Piped to yard/plot',\n",
    "    'Public tap/standpipe',\n",
    "    'TUBE WELL WATER',\n",
    "    'Tube well or borehole',\n",
    "    'Protected well',\n",
    "    'Protected spring',\n",
    "    'Rainwater',\n",
    "    'Bottled water',\n",
    "}\n",
    "\n",
    "# Calculate the fraction of households with access to fresh water for each cluster\n",
    "fraction_with_fresh_water_df = hr_df.groupby(cluster_id)['water_access'].apply(\n",
    "    lambda x: (x.isin(fresh_water_categories)).mean()\n",
    ").reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "fraction_with_fresh_water_df.columns = [cluster_id, TargetType.FRACTION_WITH_FRESH_WATER.value]\n",
    "\n",
    "# Filter out erroneous cluster IDs\n",
    "fraction_with_fresh_water_df = remove_clusters(fraction_with_fresh_water_df, \n",
    "                                               erroneous_cluster_ids, \n",
    "                                               cluster_id_column=cluster_id)\n",
    "\n",
    "# Display the result\n",
    "print(fraction_with_fresh_water_df)\n",
    "print(fraction_with_fresh_water_df.shape[0])\n",
    "print(fraction_with_fresh_water_df.index.nunique())\n",
    "\n",
    "# Compute the average fraction across all clusters\n",
    "average_fraction_with_fresh_water = fraction_with_fresh_water_df[TargetType.FRACTION_WITH_FRESH_WATER.value].mean()\n",
    "\n",
    "# Display the result as a percentage\n",
    "print(f\"Average Fraction of Households with Fresh Water Access across all clusters: {average_fraction_with_fresh_water:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DHS: Electricity Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cluster_id, fraction_with_electricity]\n",
      "Index: []\n",
      "0\n",
      "0\n",
      "Average Fraction of Households with Electricity across all clusters: nan%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the fraction of households with electricity for each cluster and create a DataFrame\n",
    "fraction_with_electricity_df = hr_df.groupby(cluster_id)['electricity_access'].apply(lambda x: (x == 'Yes').mean()).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "fraction_with_electricity_df.columns = [cluster_id, TargetType.FRACTION_WITH_ELECTRICITY.value]\n",
    "\n",
    "# Filter out erroneous cluster IDs\n",
    "fraction_with_electricity_df = remove_clusters(fraction_with_electricity_df, \n",
    "                                               erroneous_cluster_ids, \n",
    "                                               cluster_id_column=cluster_id)\n",
    "\n",
    "print(fraction_with_electricity_df)\n",
    "print(fraction_with_electricity_df.shape[0])\n",
    "print(fraction_with_electricity_df.index.nunique())\n",
    "\n",
    "# Compute the average fraction across all clusters\n",
    "average_fraction_with_electricity = fraction_with_electricity_df[TargetType.FRACTION_WITH_ELECTRICITY.value].mean()\n",
    "\n",
    "# Display the result as a percentage\n",
    "print(f\"Average Fraction of Households with Electricity across all clusters: {average_fraction_with_electricity:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cluster_id, water_access, electricity_access, radio_access, tv_access, wealth_index]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(hr_df.head())  # Check if the columns were renamed correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DHS: Radio Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cluster_id, fraction_with_radio]\n",
      "Index: []\n",
      "0\n",
      "0\n",
      "Average Fraction of Households with Electricity across all clusters: nan%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the fraction of households with radio for each cluster and create a DataFrame\n",
    "fraction_with_radio_df = hr_df.groupby(cluster_id)['radio_access'].apply(lambda x: (x == 'Yes').mean()).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "fraction_with_radio_df.columns = [cluster_id, TargetType.FRACTION_WITH_RADIO.value]\n",
    "\n",
    "# Filter out erroneous cluster IDs\n",
    "fraction_with_radio_df = remove_clusters(fraction_with_radio_df, \n",
    "                                         erroneous_cluster_ids, \n",
    "                                         cluster_id_column=cluster_id)\n",
    "\n",
    "print(fraction_with_radio_df)\n",
    "print(fraction_with_radio_df.shape[0])\n",
    "print(fraction_with_radio_df.index.nunique())\n",
    "\n",
    "# Compute the average fraction across all clusters\n",
    "average_fraction_with_radio = fraction_with_radio_df[TargetType.FRACTION_WITH_RADIO.value].mean()\n",
    "\n",
    "# Display the result as a percentage\n",
    "print(f\"Average Fraction of Households with Electricity across all clusters: {average_fraction_with_radio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DHS: TV Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cluster_id, fraction_with_tv]\n",
      "Index: []\n",
      "0\n",
      "0\n",
      "Average Fraction of Households with Electricity across all clusters: nan%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the fraction of households with TV for each cluster and create a DataFrame\n",
    "fraction_with_tv_df = hr_df.groupby(cluster_id)['tv_access'].apply(lambda x: (x == 'Yes').mean()).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "fraction_with_tv_df.columns = [cluster_id, TargetType.FRACTION_WITH_TV.value]\n",
    "\n",
    "# Filter out erroneous cluster IDs\n",
    "fraction_with_tv_df = remove_clusters(fraction_with_tv_df, \n",
    "                                      erroneous_cluster_ids, \n",
    "                                      cluster_id_column=cluster_id)\n",
    "\n",
    "print(fraction_with_tv_df)\n",
    "print(fraction_with_tv_df.shape[0])\n",
    "print(fraction_with_tv_df.index.nunique())\n",
    "\n",
    "# Compute the average fraction across all clusters\n",
    "average_fraction_with_tv = fraction_with_tv_df[TargetType.FRACTION_WITH_TV.value].mean()\n",
    "\n",
    "# Display the result as a percentage\n",
    "print(f\"Average Fraction of Households with Electricity across all clusters: {average_fraction_with_tv:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DHS: Wealth Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cluster_id, water_access, electricity_access, radio_access, tv_access, wealth_index]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Define the mapping for wealth index categories to floating-point values\n",
    "wealth_index_mapping = {\n",
    "    'Poorest': 0.0,\n",
    "    'Poorer':  0.25,\n",
    "    'Middle':  0.5,\n",
    "    'Richer':  0.75,\n",
    "    'Richest': 1.0\n",
    "}\n",
    "\n",
    "# Replace original wealth index categories with corresponding floating-point values and convert to float\n",
    "hr_df['wealth_index'] = hr_df['wealth_index'].map(wealth_index_mapping).astype(float)\n",
    "print(hr_df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cluster_id, mean_wealth_index]\n",
      "Index: []\n",
      "0\n",
      "0\n",
      "Average Fraction of Households with Electricity across all clusters: nan%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean wealth index for each cluster\n",
    "mean_wealth_by_cluster_df = hr_df.groupby(cluster_id)['wealth_index'].mean().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "mean_wealth_by_cluster_df.columns = [cluster_id, TargetType.MEAN_WEALTH_INDEX.value]\n",
    "\n",
    "\n",
    "# Filter out erroneous cluster IDs\n",
    "mean_wealth_by_cluster_df = remove_clusters(mean_wealth_by_cluster_df, \n",
    "                                            erroneous_cluster_ids, \n",
    "                                            cluster_id_column=cluster_id)\n",
    "\n",
    "# Display the table\n",
    "print(mean_wealth_by_cluster_df)\n",
    "print(mean_wealth_by_cluster_df.shape[0])\n",
    "print(mean_wealth_by_cluster_df.index.nunique())\n",
    "\n",
    "# Compute the average fraction across all clusters\n",
    "average_wealth_index = mean_wealth_by_cluster_df[TargetType.MEAN_WEALTH_INDEX.value].mean()\n",
    "\n",
    "# Display the result as a percentage\n",
    "print(f\"Average Fraction of Households with Electricity across all clusters: {average_wealth_index:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DHS Child Recode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Loaded Successfully\n",
      "   V001   H7   H8   H9\n",
      "0   1.0  0.0  0.0  0.0\n",
      "1   1.0  2.0  2.0  2.0\n",
      "2   1.0  NaN  NaN  NaN\n",
      "3   1.0  2.0  2.0  2.0\n",
      "4   2.0  2.0  2.0  2.0\n",
      "5   2.0  1.0  1.0  0.0\n",
      "6   2.0  NaN  NaN  NaN\n",
      "7   2.0  NaN  NaN  NaN\n",
      "8   2.0  1.0  1.0  1.0\n",
      "9   3.0  1.0  1.0  1.0\n",
      "10658\n",
      "Updated DataFrame:\n",
      "    V001   H7   H8   H9\n",
      "0    1.0   No   No   No\n",
      "5    2.0  Yes  Yes   No\n",
      "8    2.0  Yes  Yes  Yes\n",
      "9    3.0  Yes  Yes  Yes\n",
      "14   5.0  Yes  Yes  Yes\n",
      "15   5.0  Yes   No   No\n",
      "16   5.0   No   No   No\n",
      "17   5.0   No   No   No\n",
      "19   5.0   No   No   No\n",
      "20   5.0   No   No   No\n",
      "5124\n"
     ]
    }
   ],
   "source": [
    "selected_columns = list(kr_column_mapping.keys())\n",
    "\n",
    "# Load the selected columns\n",
    "kr_df, meta = pyreadstat.read_sav(recode_kr_path, usecols=selected_columns)\n",
    "\n",
    "\n",
    "# Ensure the DataFrame loaded properly\n",
    "if kr_df is not None:\n",
    "    print(\"DataFrame Loaded Successfully\")\n",
    "    print(kr_df.head(10))\n",
    "    print(len(kr_df))\n",
    "\n",
    "# Define the labels for DPT vaccination (or other relevant columns)\n",
    "dpt_labels = {\n",
    "    0.0: 'No',\n",
    "    1.0: 'Yes',\n",
    "}\n",
    "\n",
    "# Apply the mappings to convert numeric values to labels\n",
    "kr_df['H7'] = kr_df['H7'].map(dpt_labels)\n",
    "kr_df['H8'] = kr_df['H8'].map(dpt_labels)\n",
    "kr_df['H9'] = kr_df['H9'].map(dpt_labels)\n",
    "\n",
    "# Drop rows with NaN\n",
    "kr_df.dropna(subset=['H7', 'H8', 'H9'], inplace=True)\n",
    "\n",
    "\n",
    "# Print the updated DataFrame to confirm the mappings and clean data\n",
    "print(\"Updated DataFrame:\")\n",
    "print(kr_df.head(10))\n",
    "print(len(kr_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cluster_id dpt1 dpt2 dpt3\n",
      "0          1.0   No   No   No\n",
      "5          2.0  Yes  Yes   No\n",
      "8          2.0  Yes  Yes  Yes\n",
      "9          3.0  Yes  Yes  Yes\n",
      "14         5.0  Yes  Yes  Yes\n"
     ]
    }
   ],
   "source": [
    "kr_df.rename(columns=kr_column_mapping, inplace=True)\n",
    "print(kr_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DHS: DPT3 Vaccination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cluster_id\n",
      "count  5124.000000\n",
      "mean    510.110656\n",
      "std     260.471154\n",
      "min       1.000000\n",
      "25%     300.750000\n",
      "50%     521.000000\n",
      "75%     724.000000\n",
      "max     970.000000\n",
      "\n",
      "\n",
      "    cluster_id dpt1 dpt2 dpt3\n",
      "0          1.0   No   No   No\n",
      "5          2.0  Yes  Yes   No\n",
      "8          2.0  Yes  Yes  Yes\n",
      "9          3.0  Yes  Yes  Yes\n",
      "14         5.0  Yes  Yes  Yes\n",
      "\n",
      "\n",
      "Average number of survey points per cluster:  5.5\n"
     ]
    }
   ],
   "source": [
    "# Inspect DataFrame\n",
    "# print(kr_df.info())  # General info about the DataFrame\n",
    "print(kr_df.describe())  # Summary statistics of numeric columns (if any)\n",
    "print(\"\\n\")\n",
    "print(kr_df.head())  # Preview the first few rows of the DataFrame\n",
    "print(\"\\n\")\n",
    "# Calculate and print the average number of survey points per cluster\n",
    "cluster_sizes = kr_df.groupby(cluster_id).size()\n",
    "average_survey_points_per_cluster = cluster_sizes.mean()\n",
    "print(f\"Average number of survey points per cluster:  {format(average_survey_points_per_cluster, '.1f')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cluster_id  fraction_dpt3_vaccinated\n",
      "0           1.0                  0.000000\n",
      "1           2.0                  0.500000\n",
      "2           3.0                  1.000000\n",
      "3           5.0                  0.142857\n",
      "4           6.0                  0.333333\n",
      "..          ...                       ...\n",
      "924       966.0                  0.625000\n",
      "925       967.0                  0.250000\n",
      "926       968.0                  0.750000\n",
      "927       969.0                  0.470588\n",
      "928       970.0                  0.250000\n",
      "\n",
      "[929 rows x 2 columns]\n",
      "Final DataFrame shape: 929\n",
      "Unique clusters in index: 929\n",
      "Average Fraction of fully vaccinated children across all clusters: 45.34%\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all clusters\n",
    "all_clusters = kr_df[cluster_id].unique()\n",
    "\n",
    "# Filter children who have \"Yes\" for all three doses\n",
    "fully_vaccinated = kr_df[\n",
    "    (kr_df['dpt1'] == 'Yes') & (kr_df['dpt2'] == 'Yes') & (kr_df['dpt3'] == 'Yes')\n",
    "]\n",
    "\n",
    "# Calculate the numerator: number of fully vaccinated children per cluster\n",
    "numerator = fully_vaccinated.groupby(cluster_id).size()\n",
    "\n",
    "# Reindex numerator to include all clusters, filling missing values with 0\n",
    "numerator = numerator.reindex(all_clusters, fill_value=0)\n",
    "\n",
    "# Calculate the denominator: total number of children per cluster\n",
    "denominator = kr_df.groupby(cluster_id).size()\n",
    "\n",
    "# Ensure the denominator includes all clusters\n",
    "denominator = denominator.reindex(all_clusters, fill_value=0)\n",
    "\n",
    "# Compute the fraction of fully vaccinated children per cluster\n",
    "fraction_dpt3_vaccinated = numerator / denominator\n",
    "\n",
    "# Convert the series to DataFrame and reset index\n",
    "fraction_dpt3_vaccinated_df = fraction_dpt3_vaccinated.reset_index()\n",
    "\n",
    "# Rename columns appropriately\n",
    "fraction_dpt3_vaccinated_df.columns = [cluster_id, TargetType.FRACTION_DPT3_VACCINATED.value]\n",
    "\n",
    "# Fill any NaN values with 0 (in case of division by zero)\n",
    "fraction_dpt3_vaccinated_df.fillna(0, inplace=True)\n",
    "\n",
    "# Remove erroneous clusters if needed\n",
    "fraction_dpt3_vaccinated_df = remove_clusters(fraction_dpt3_vaccinated_df,\n",
    "                                              erroneous_cluster_ids,\n",
    "                                              cluster_id_column=cluster_id)\n",
    "\n",
    "# Final debug: Print and inspect the DataFrame\n",
    "print(fraction_dpt3_vaccinated_df)\n",
    "print(f\"Final DataFrame shape: {fraction_dpt3_vaccinated_df.shape[0]}\")\n",
    "print(f\"Unique clusters in index: {fraction_dpt3_vaccinated_df.index.nunique()}\")\n",
    "\n",
    "# Compute the average fraction across all clusters\n",
    "average_fraction_dpt3_vaccinated = fraction_dpt3_vaccinated_df[TargetType.FRACTION_DPT3_VACCINATED.value].mean()\n",
    "\n",
    "# Display the result as a percentage\n",
    "print(f\"Average Fraction of fully vaccinated children across all clusters: {average_fraction_dpt3_vaccinated:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the detailed individual survey responses to a CSV file for manual spot check\n",
    "# kr_df.to_csv(f'{country_code}_survey_responses_aoi.csv', index=False)\n",
    "\n",
    "# # Print a message indicating the file was created\n",
    "# print(\"Detailed survey data saved to 'survey_responses_aoi.csv' for manual inspection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add DHS Target Values to Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df[cluster_id] = cluster_df[cluster_id].astype(float).astype(int).astype(str)\n",
    "fraction_dpt3_vaccinated_df[cluster_id]  = fraction_dpt3_vaccinated_df[cluster_id].astype(float).astype(int).astype(str)\n",
    "fraction_with_electricity_df[cluster_id] = fraction_with_electricity_df[cluster_id].astype(float).astype(int).astype(str)\n",
    "fraction_with_fresh_water_df[cluster_id] = fraction_with_fresh_water_df[cluster_id].astype(float).astype(int).astype(str)\n",
    "fraction_with_tv_df[cluster_id]          = fraction_with_tv_df[cluster_id].astype(float).astype(int).astype(str)\n",
    "fraction_with_radio_df[cluster_id]       = fraction_with_radio_df[cluster_id].astype(float).astype(int).astype(str)\n",
    "mean_wealth_by_cluster_df[cluster_id]    = mean_wealth_by_cluster_df[cluster_id].astype(float).astype(int).astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values detected. Here's the count of NaNs per column:\n",
      "cluster_id                     0\n",
      "lat                            0\n",
      "lon                            0\n",
      "fraction_dpt3_vaccinated      41\n",
      "fraction_with_electricity    970\n",
      "fraction_with_fresh_water    970\n",
      "fraction_with_tv             970\n",
      "fraction_with_radio          970\n",
      "mean_wealth_index            970\n",
      "dtype: int64\n",
      "Removing clusters with missing data.\n",
      "Total number of clusters removed: 970\n",
      "No NaN values after removal.\n"
     ]
    }
   ],
   "source": [
    "# Merge with outer join and check for NaNs after each step\n",
    "dhs_df = pd.merge(cluster_df, fraction_dpt3_vaccinated_df,  on=cluster_id, how='outer')\n",
    "dhs_df = pd.merge(dhs_df,     fraction_with_electricity_df, on=cluster_id, how='outer')\n",
    "dhs_df = pd.merge(dhs_df,     fraction_with_fresh_water_df, on=cluster_id, how='outer')\n",
    "dhs_df = pd.merge(dhs_df,     fraction_with_tv_df,          on=cluster_id, how='outer')\n",
    "dhs_df = pd.merge(dhs_df,     fraction_with_radio_df,       on=cluster_id, how='outer')\n",
    "dhs_df = pd.merge(dhs_df,     mean_wealth_by_cluster_df,    on=cluster_id, how='outer')\n",
    "\n",
    "# Store the initial number of clusters (rows) before removing NaNs\n",
    "initial_num_clusters = dhs_df.shape[0]\n",
    "\n",
    "# Check if NaNs exist and list the number of NaNs per column\n",
    "if dhs_df.isnull().sum().sum() > 0:\n",
    "    print(\"NaN values detected. Here's the count of NaNs per column:\")\n",
    "    print(dhs_df.isnull().sum())  # Shows the number of NaN values per column\n",
    "    \n",
    "    # Remove rows with any NaN values\n",
    "    print(\"Removing clusters with missing data.\")\n",
    "    dhs_df = dhs_df.dropna()\n",
    "\n",
    "    # Store the number of clusters removed\n",
    "    final_num_clusters = dhs_df.shape[0]\n",
    "    num_clusters_removed = initial_num_clusters - final_num_clusters\n",
    "\n",
    "    print(f\"Total number of clusters removed: {num_clusters_removed}\")\n",
    "else:\n",
    "    print(\"No NaN values detected. No removal necessary.\")\n",
    "    num_clusters_removed = 0\n",
    "\n",
    "# Final debug: Ensure no NaNs remain after removal\n",
    "if dhs_df.isnull().sum().sum() > 0:\n",
    "    print(f\"NaN values still present after removal: {dhs_df.isnull().sum()}\")\n",
    "else:\n",
    "    print(\"No NaN values after removal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cluster_id, lat, lon, fraction_dpt3_vaccinated, fraction_with_electricity, fraction_with_fresh_water, fraction_with_tv, fraction_with_radio, mean_wealth_index]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(dhs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cluster_id, lat, lon, fraction_dpt3_vaccinated, fraction_with_electricity, fraction_with_fresh_water, fraction_with_tv, fraction_with_radio, mean_wealth_index]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Convert cluster_id back to numeric (integer) type for sorting purposes\n",
    "dhs_df[cluster_id] = dhs_df[cluster_id].astype(float).astype(int)\n",
    "\n",
    "# Sort the DataFrame by cluster_id in ascending numerical order\n",
    "dhs_df = dhs_df.sort_values(by=cluster_id)\n",
    "\n",
    "print(dhs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `targets.json` to Store Cluster Data and Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_values_json(dhs_df, country_code, target_types, target_averages, num_clusters_removed, output_file='targets.json'):\n",
    "    \"\"\"\n",
    "    Generates a JSON file containing target values along with cluster latitude and longitude.\n",
    "    Adds metadata specifying the number of records, a description, the country code, and the number of clusters removed.\n",
    "\n",
    "    Parameters:\n",
    "        dhs_df (pd.DataFrame): The DataFrame containing cluster data including 'cluster_id', 'lat', 'lon', and target values.\n",
    "        target_types (list): List of target type strings to include in the JSON.\n",
    "        target_averages (dict): Dictionary of target types and their average values for the entire AOI.\n",
    "        num_clusters_removed (int): The number of clusters that were removed due to missing data.\n",
    "        country_code (str): The country code to include in the metadata.\n",
    "        output_file (str): The path to save the output JSON file. Default is 'targets.json'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_dir) and output_dir != '':\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created directory {output_dir}\")\n",
    "    \n",
    "    # Convert the DataFrame to a dictionary with cluster_id as the key\n",
    "    target_values_dict = {}\n",
    "    for _, row in dhs_df.iterrows():\n",
    "        cluster_id = int(row['cluster_id'])  # Convert cluster_id to int\n",
    "        cluster_entry = {\n",
    "            'lat': round(row['lat'], 3), \n",
    "            'lon': round(row['lon'], 3)\n",
    "        }\n",
    "        for target in target_types:\n",
    "            # Ensure the target exists in the row to avoid KeyError\n",
    "            if target in row:\n",
    "                cluster_entry[target] = round(row[target], 3)\n",
    "            else:\n",
    "                # Assign None or an appropriate default if the target is missing\n",
    "                cluster_entry[target] = None\n",
    "        target_values_dict[cluster_id] = cluster_entry\n",
    "        \n",
    "        target_averages = {key: round(value, 3) for key, value in target_averages.items()}\n",
    "\n",
    "\n",
    "    # Count the number of records\n",
    "    num_records = len(target_values_dict)\n",
    "\n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        \"metadata\": {\n",
    "            \"country_code\": country_code,  # Include the country code\n",
    "            \"num_records\": num_records,\n",
    "            \"num_clusters_removed\": num_clusters_removed,  # Add the number of clusters removed\n",
    "            \"description\": \"This file contains target values for vaccination, electricity, fresh water, and wealth index for each cluster. Clusters with missing data were removed.\",\n",
    "            \"average_target_values\": target_averages  # Include the averages in the metadata\n",
    "        },\n",
    "        \"clusters\": target_values_dict  # Include the target values data under 'clusters'\n",
    "    }\n",
    "\n",
    "    # Save the JSON file with metadata and target values\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    \n",
    "    print(f\"Target values JSON saved to {output_file} with {num_records} records. {num_clusters_removed} clusters were removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if CREATE_TARGETS:\n",
    "    \n",
    "    target_types = [\n",
    "        TargetType.FRACTION_DPT3_VACCINATED.value,\n",
    "        TargetType.FRACTION_WITH_ELECTRICITY.value,\n",
    "        TargetType.FRACTION_WITH_FRESH_WATER.value,\n",
    "        TargetType.MEAN_WEALTH_INDEX.value,\n",
    "        TargetType.FRACTION_WITH_RADIO.value,\n",
    "        TargetType.FRACTION_WITH_TV.value\n",
    "    ]\n",
    "\n",
    "    target_averages = {\n",
    "        TargetType.FRACTION_DPT3_VACCINATED.value: average_fraction_dpt3_vaccinated,\n",
    "        TargetType.FRACTION_WITH_ELECTRICITY.value: average_fraction_with_electricity,\n",
    "        TargetType.FRACTION_WITH_FRESH_WATER.value: average_fraction_with_fresh_water,\n",
    "        TargetType.MEAN_WEALTH_INDEX.value: average_wealth_index,\n",
    "        TargetType.FRACTION_WITH_RADIO.value: average_fraction_with_radio,\n",
    "        TargetType.FRACTION_WITH_TV.value: average_fraction_with_tv\n",
    "    }\n",
    "\n",
    "    generate_target_values_json(dhs_df, \n",
    "                                country_code, \n",
    "                                target_types, \n",
    "                                target_averages,\n",
    "                                num_clusters_removed, \n",
    "                                target_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "geospatial_df = dhs_df.copy()\n",
    "    \n",
    "if CREATE_DHS_MAPS:\n",
    "\n",
    "    mercator_x, mercator_y = wgs84_to_mercator(geospatial_df['lon'].values, geospatial_df['lat'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[288], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m     tooltips\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNightlights (mean)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@nightlights_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     41\u001b[0m reversed_palette \u001b[38;5;241m=\u001b[39m Viridis256[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     42\u001b[0m color_mapper \u001b[38;5;241m=\u001b[39m LinearColorMapper(palette\u001b[38;5;241m=\u001b[39mreversed_palette, \n\u001b[0;32m---> 43\u001b[0m                                   low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfraction_dpt3_vaccinated\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     44\u001b[0m                                   high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(source\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraction_dpt3_vaccinated\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     46\u001b[0m color_spec \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraction_dpt3_vaccinated\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m: color_mapper\n\u001b[1;32m     49\u001b[0m }\n\u001b[1;32m     52\u001b[0m plot_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Vaccination Coverage\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "if CREATE_DHS_MAPS:\n",
    "    \n",
    "    # Initialize the data dictionary with mandatory fields\n",
    "    data_dict = {\n",
    "        'cluster_id': geospatial_df.index,\n",
    "        'lat': geospatial_df['lat'],\n",
    "        'lon': geospatial_df['lon'],\n",
    "        'fraction_dpt3_vaccinated': geospatial_df['fraction_dpt3_vaccinated'],\n",
    "        'mercator_x': mercator_x,\n",
    "        'mercator_y': mercator_y\n",
    "    }\n",
    "\n",
    "    # Add optional fields if they exist in the dataframe\n",
    "    if 'rainfall_mean' in geospatial_df.columns:\n",
    "        data_dict['rainfall_mean'] = geospatial_df['rainfall_mean']\n",
    "    if 'population_mean' in geospatial_df.columns:\n",
    "        data_dict['population_mean'] = geospatial_df['population_mean']\n",
    "    if 'nightlights_mean' in geospatial_df.columns:\n",
    "        data_dict['nightlights_mean'] = geospatial_df['nightlights_mean']\n",
    "\n",
    "    # Create the ColumnDataSource with the dynamically constructed data_dict\n",
    "    source = ColumnDataSource(data=data_dict)\n",
    "\n",
    "    # Define mandatory tooltips with percentage format for DPT3 Vaccinated\n",
    "    tooltips = [\n",
    "        (\"Cluster ID\", \"@cluster_id\"),\n",
    "        (\"Latitude\", \"@lat\"),\n",
    "        (\"Longitude\", \"@lon\"),\n",
    "        (\"DPT3 Vaccinated\", \"@fraction_dpt3_vaccinated{0.0%}\")\n",
    "    ]\n",
    "\n",
    "    # Conditionally add optional tooltips based on available data\n",
    "    if 'rainfall_mean' in data_dict:\n",
    "        tooltips.append((\"Rainfall (mean)\", \"@rainfall_mean\"))\n",
    "    if 'population_mean' in data_dict:\n",
    "        tooltips.append((\"Population (mean)\", \"@population_mean\"))\n",
    "    if 'nightlights_mean' in data_dict:\n",
    "        tooltips.append((\"Nightlights (mean)\", \"@nightlights_mean\"))\n",
    "\n",
    "\n",
    "    reversed_palette = Viridis256[::-1]\n",
    "    color_mapper = LinearColorMapper(palette=reversed_palette, \n",
    "                                      low=min(source.data['fraction_dpt3_vaccinated']), \n",
    "                                      high=max(source.data['fraction_dpt3_vaccinated']))\n",
    "\n",
    "    color_spec = {\n",
    "        'field': 'fraction_dpt3_vaccinated', \n",
    "        'transform': color_mapper\n",
    "    }\n",
    "\n",
    "\n",
    "    plot_title = f'{country_code} Vaccination Coverage'\n",
    "\n",
    "    color_bar_title = \"Vaccination Coverage\"\n",
    "\n",
    "    case = f'{country_code}_Vaccination_Coverage'\n",
    "\n",
    "    out_dir = results_config.PLOT_GEOSPATIAL_DIR\n",
    "\n",
    "    create_geospatial_plot(source, \n",
    "                           tooltips, \n",
    "                           color_spec, \n",
    "                           out_dir, \n",
    "                           case=case,\n",
    "                           plot_title=plot_title,\n",
    "                           color_bar=True, \n",
    "                           color_bar_title=color_bar_title,\n",
    "                           symbol_size=symbol_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[289], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m     tooltips\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNightlights (mean)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@nightlights_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     42\u001b[0m reversed_palette \u001b[38;5;241m=\u001b[39m Viridis256[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m color_mapper \u001b[38;5;241m=\u001b[39m LinearColorMapper(palette\u001b[38;5;241m=\u001b[39mreversed_palette, \n\u001b[0;32m---> 44\u001b[0m                                   low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfraction_with_electricity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     45\u001b[0m                                   high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(source\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraction_with_electricity\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     47\u001b[0m color_spec \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraction_with_electricity\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m: color_mapper\n\u001b[1;32m     50\u001b[0m }\n\u001b[1;32m     53\u001b[0m plot_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Electricty Coverage\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "if CREATE_DHS_MAPS:\n",
    "    \n",
    "    # Initialize the data dictionary with mandatory fields\n",
    "    data_dict = {\n",
    "        'cluster_id': geospatial_df.index,\n",
    "        'lat': geospatial_df['lat'],\n",
    "        'lon': geospatial_df['lon'],\n",
    "        'fraction_with_electricity': geospatial_df['fraction_with_electricity'],\n",
    "        'mercator_x': mercator_x,\n",
    "        'mercator_y': mercator_y\n",
    "    }\n",
    "\n",
    "    # Add optional fields if they exist in the dataframe\n",
    "    if 'rainfall_mean' in geospatial_df.columns:\n",
    "        data_dict['rainfall_mean'] = geospatial_df['rainfall_mean']\n",
    "    if 'population_mean' in geospatial_df.columns:\n",
    "        data_dict['population_mean'] = geospatial_df['population_mean']\n",
    "    if 'nightlights_mean' in geospatial_df.columns:\n",
    "        data_dict['nightlights_mean'] = geospatial_df['nightlights_mean']\n",
    "\n",
    "    # Create the ColumnDataSource with the dynamically constructed data_dict\n",
    "    source = ColumnDataSource(data=data_dict)\n",
    "\n",
    "    # Define mandatory tooltips with percentage format for Electricity Access\n",
    "    tooltips = [\n",
    "        (\"Cluster ID\", \"@cluster_id\"),\n",
    "        (\"Latitude\", \"@lat\"),\n",
    "        (\"Longitude\", \"@lon\"),\n",
    "        (\"Electricity Access\", \"@fraction_with_electricity{0.0%}\")\n",
    "    ]\n",
    "\n",
    "    # Conditionally add optional tooltips based on available data\n",
    "    if 'rainfall_mean' in data_dict:\n",
    "        tooltips.append((\"Rainfall (mean)\", \"@rainfall_mean\"))\n",
    "    if 'population_mean' in data_dict:\n",
    "        tooltips.append((\"Population (mean)\", \"@population_mean\"))\n",
    "    if 'nightlights_mean' in data_dict:\n",
    "        tooltips.append((\"Nightlights (mean)\", \"@nightlights_mean\"))\n",
    "\n",
    "\n",
    "\n",
    "    reversed_palette = Viridis256[::-1]\n",
    "    color_mapper = LinearColorMapper(palette=reversed_palette, \n",
    "                                      low=min(source.data['fraction_with_electricity']), \n",
    "                                      high=max(source.data['fraction_with_electricity']))\n",
    "\n",
    "    color_spec = {\n",
    "        'field': 'fraction_with_electricity', \n",
    "        'transform': color_mapper\n",
    "    }\n",
    "\n",
    "\n",
    "    plot_title = f'{country_code} Electricty Coverage'\n",
    "\n",
    "    color_bar_title = \"Electricty Coverage\"\n",
    "\n",
    "    case = f'{country_code}_Electricty_Coverage'\n",
    "\n",
    "    out_dir = results_config.PLOT_GEOSPATIAL_DIR\n",
    "\n",
    "    create_geospatial_plot(source, \n",
    "                           tooltips, \n",
    "                           color_spec, \n",
    "                           out_dir, \n",
    "                           case=case,\n",
    "                           plot_title=plot_title,\n",
    "                           color_bar=True, \n",
    "                           color_bar_title=color_bar_title,\n",
    "                           symbol_size=symbol_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[290], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m     tooltips\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNightlights (mean)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@nightlights_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     42\u001b[0m reversed_palette \u001b[38;5;241m=\u001b[39m Viridis256[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m color_mapper \u001b[38;5;241m=\u001b[39m LinearColorMapper(palette\u001b[38;5;241m=\u001b[39mreversed_palette, \n\u001b[0;32m---> 44\u001b[0m                                   low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfraction_with_radio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     45\u001b[0m                                   high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(source\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraction_with_radio\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     47\u001b[0m color_spec \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraction_with_radio\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m: color_mapper\n\u001b[1;32m     50\u001b[0m }\n\u001b[1;32m     53\u001b[0m plot_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Radio Coverage\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "if CREATE_DHS_MAPS:\n",
    "    \n",
    "    # Initialize the data dictionary with mandatory fields\n",
    "    data_dict = {\n",
    "        'cluster_id': geospatial_df.index,\n",
    "        'lat': geospatial_df['lat'],\n",
    "        'lon': geospatial_df['lon'],\n",
    "        'fraction_with_radio': geospatial_df['fraction_with_radio'],\n",
    "        'mercator_x': mercator_x,\n",
    "        'mercator_y': mercator_y\n",
    "    }\n",
    "\n",
    "    # Add optional fields if they exist in the dataframe\n",
    "    if 'rainfall_mean' in geospatial_df.columns:\n",
    "        data_dict['rainfall_mean'] = geospatial_df['rainfall_mean']\n",
    "    if 'population_mean' in geospatial_df.columns:\n",
    "        data_dict['population_mean'] = geospatial_df['population_mean']\n",
    "    if 'nightlights_mean' in geospatial_df.columns:\n",
    "        data_dict['nightlights_mean'] = geospatial_df['nightlights_mean']\n",
    "\n",
    "    # Create the ColumnDataSource with the dynamically constructed data_dict\n",
    "    source = ColumnDataSource(data=data_dict)\n",
    "\n",
    "    # Define mandatory tooltips with percentage format for Electricity Access\n",
    "    tooltips = [\n",
    "        (\"Cluster ID\", \"@cluster_id\"),\n",
    "        (\"Latitude\", \"@lat\"),\n",
    "        (\"Longitude\", \"@lon\"),\n",
    "        (\"Radio Access\", \"@fraction_with_radio{0.0%}\")\n",
    "    ]\n",
    "\n",
    "    # Conditionally add optional tooltips based on available data\n",
    "    if 'rainfall_mean' in data_dict:\n",
    "        tooltips.append((\"Rainfall (mean)\", \"@rainfall_mean\"))\n",
    "    if 'population_mean' in data_dict:\n",
    "        tooltips.append((\"Population (mean)\", \"@population_mean\"))\n",
    "    if 'nightlights_mean' in data_dict:\n",
    "        tooltips.append((\"Nightlights (mean)\", \"@nightlights_mean\"))\n",
    "\n",
    "\n",
    "\n",
    "    reversed_palette = Viridis256[::-1]\n",
    "    color_mapper = LinearColorMapper(palette=reversed_palette, \n",
    "                                      low=min(source.data['fraction_with_radio']), \n",
    "                                      high=max(source.data['fraction_with_radio']))\n",
    "\n",
    "    color_spec = {\n",
    "        'field': 'fraction_with_radio', \n",
    "        'transform': color_mapper\n",
    "    }\n",
    "\n",
    "\n",
    "    plot_title = f'{country_code} Radio Coverage'\n",
    "\n",
    "    color_bar_title = \"Radio Coverage\"\n",
    "\n",
    "    case = f'{country_code}_Radio_Coverage'\n",
    "\n",
    "    out_dir = results_config.PLOT_GEOSPATIAL_DIR\n",
    "\n",
    "    create_geospatial_plot(source, \n",
    "                           tooltips, \n",
    "                           color_spec, \n",
    "                           out_dir, \n",
    "                           case=case,\n",
    "                           plot_title=plot_title,\n",
    "                           color_bar=True, \n",
    "                           color_bar_title=color_bar_title,\n",
    "                           symbol_size=symbol_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[291], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m     tooltips\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNightlights (mean)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@nightlights_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     42\u001b[0m reversed_palette \u001b[38;5;241m=\u001b[39m Viridis256[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m color_mapper \u001b[38;5;241m=\u001b[39m LinearColorMapper(palette\u001b[38;5;241m=\u001b[39mreversed_palette, \n\u001b[0;32m---> 44\u001b[0m                                   low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfraction_with_tv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     45\u001b[0m                                   high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(source\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraction_with_tv\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     47\u001b[0m color_spec \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraction_with_tv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m: color_mapper\n\u001b[1;32m     50\u001b[0m }\n\u001b[1;32m     53\u001b[0m plot_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m TV Coverage\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "if CREATE_DHS_MAPS:\n",
    "\n",
    "    # Initialize the data dictionary with mandatory fields\n",
    "    data_dict = {\n",
    "        'cluster_id': geospatial_df.index,\n",
    "        'lat': geospatial_df['lat'],\n",
    "        'lon': geospatial_df['lon'],\n",
    "        'fraction_with_tv': geospatial_df['fraction_with_tv'],\n",
    "        'mercator_x': mercator_x,\n",
    "        'mercator_y': mercator_y\n",
    "    }\n",
    "\n",
    "    # Add optional fields if they exist in the dataframe\n",
    "    if 'rainfall_mean' in geospatial_df.columns:\n",
    "        data_dict['rainfall_mean'] = geospatial_df['rainfall_mean']\n",
    "    if 'population_mean' in geospatial_df.columns:\n",
    "        data_dict['population_mean'] = geospatial_df['population_mean']\n",
    "    if 'nightlights_mean' in geospatial_df.columns:\n",
    "        data_dict['nightlights_mean'] = geospatial_df['nightlights_mean']\n",
    "\n",
    "    # Create the ColumnDataSource with the dynamically constructed data_dict\n",
    "    source = ColumnDataSource(data=data_dict)\n",
    "\n",
    "    # Define mandatory tooltips with percentage format for Electricity Access\n",
    "    tooltips = [\n",
    "        (\"Cluster ID\", \"@cluster_id\"),\n",
    "        (\"Latitude\", \"@lat\"),\n",
    "        (\"Longitude\", \"@lon\"),\n",
    "        (\"Radio Access\", \"@fraction_with_tv{0.0%}\")\n",
    "    ]\n",
    "\n",
    "    # Conditionally add optional tooltips based on available data\n",
    "    if 'rainfall_mean' in data_dict:\n",
    "        tooltips.append((\"Rainfall (mean)\", \"@rainfall_mean\"))\n",
    "    if 'population_mean' in data_dict:\n",
    "        tooltips.append((\"Population (mean)\", \"@population_mean\"))\n",
    "    if 'nightlights_mean' in data_dict:\n",
    "        tooltips.append((\"Nightlights (mean)\", \"@nightlights_mean\"))\n",
    "\n",
    "\n",
    "\n",
    "    reversed_palette = Viridis256[::-1]\n",
    "    color_mapper = LinearColorMapper(palette=reversed_palette, \n",
    "                                      low=min(source.data['fraction_with_tv']), \n",
    "                                      high=max(source.data['fraction_with_tv']))\n",
    "\n",
    "    color_spec = {\n",
    "        'field': 'fraction_with_tv', \n",
    "        'transform': color_mapper\n",
    "    }\n",
    "\n",
    "\n",
    "    plot_title = f'{country_code} TV Coverage'\n",
    "\n",
    "    color_bar_title = \"TV Coverage\"\n",
    "\n",
    "    case = f'{country_code}_TV_Coverage'\n",
    "\n",
    "    out_dir = results_config.PLOT_GEOSPATIAL_DIR\n",
    "\n",
    "    create_geospatial_plot(source, \n",
    "                           tooltips, \n",
    "                           color_spec, \n",
    "                           out_dir, \n",
    "                           case=case,\n",
    "                           plot_title=plot_title,\n",
    "                           color_bar=True, \n",
    "                           color_bar_title=color_bar_title,\n",
    "                           symbol_size=symbol_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[292], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m     tooltips\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNightlights (mean)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@nightlights_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     41\u001b[0m reversed_palette \u001b[38;5;241m=\u001b[39m Viridis256[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     42\u001b[0m color_mapper \u001b[38;5;241m=\u001b[39m LinearColorMapper(palette\u001b[38;5;241m=\u001b[39mreversed_palette, \n\u001b[0;32m---> 43\u001b[0m                                   low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfraction_with_fresh_water\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     44\u001b[0m                                   high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(source\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraction_with_fresh_water\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     46\u001b[0m color_spec \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraction_with_fresh_water\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m: color_mapper\n\u001b[1;32m     49\u001b[0m }\n\u001b[1;32m     52\u001b[0m plot_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Fresh Water Coverage\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "if CREATE_DHS_MAPS:\n",
    "\n",
    "    # Initialize the data dictionary with mandatory fields\n",
    "    data_dict = {\n",
    "        'cluster_id': geospatial_df.index,\n",
    "        'lat': geospatial_df['lat'],\n",
    "        'lon': geospatial_df['lon'],\n",
    "        'fraction_with_fresh_water': geospatial_df['fraction_with_fresh_water'],\n",
    "        'mercator_x': mercator_x,\n",
    "        'mercator_y': mercator_y\n",
    "    }\n",
    "\n",
    "    # Add optional fields if they exist in the dataframe\n",
    "    if 'rainfall_mean' in geospatial_df.columns:\n",
    "        data_dict['rainfall_mean'] = geospatial_df['rainfall_mean']\n",
    "    if 'population_mean' in geospatial_df.columns:\n",
    "        data_dict['population_mean'] = geospatial_df['population_mean']\n",
    "    if 'nightlights_mean' in geospatial_df.columns:\n",
    "        data_dict['nightlights_mean'] = geospatial_df['nightlights_mean']\n",
    "\n",
    "    # Create the ColumnDataSource with the dynamically constructed data_dict\n",
    "    source = ColumnDataSource(data=data_dict)\n",
    "\n",
    "    # Define mandatory tooltips with percentage format for Electricity Access\n",
    "    tooltips = [\n",
    "        (\"Cluster ID\", \"@cluster_id\"),\n",
    "        (\"Latitude\", \"@lat\"),\n",
    "        (\"Longitude\", \"@lon\"),\n",
    "        (\"Fresh Water Access\", \"@fraction_with_fresh_water{0.0%}\")\n",
    "    ]\n",
    "\n",
    "    # Conditionally add optional tooltips based on available data\n",
    "    if 'rainfall_mean' in data_dict:\n",
    "        tooltips.append((\"Rainfall (mean)\", \"@rainfall_mean\"))\n",
    "    if 'population_mean' in data_dict:\n",
    "        tooltips.append((\"Population (mean)\", \"@population_mean\"))\n",
    "    if 'nightlights_mean' in data_dict:\n",
    "        tooltips.append((\"Nightlights (mean)\", \"@nightlights_mean\"))\n",
    "\n",
    "\n",
    "    reversed_palette = Viridis256[::-1]\n",
    "    color_mapper = LinearColorMapper(palette=reversed_palette, \n",
    "                                      low=min(source.data['fraction_with_fresh_water']), \n",
    "                                      high=max(source.data['fraction_with_fresh_water']))\n",
    "\n",
    "    color_spec = {\n",
    "        'field': 'fraction_with_fresh_water', \n",
    "        'transform': color_mapper\n",
    "    }\n",
    "\n",
    "\n",
    "    plot_title = f'{country_code} Fresh Water Coverage'\n",
    "\n",
    "    color_bar_title = \"Fresh Water Coverage\"\n",
    "\n",
    "    case = f'{country_code}_Fresh_Water_Coverage'\n",
    "\n",
    "    out_dir = results_config.PLOT_GEOSPATIAL_DIR\n",
    "\n",
    "    create_geospatial_plot(source, \n",
    "                           tooltips, \n",
    "                           color_spec, \n",
    "                           out_dir, \n",
    "                           case=case,\n",
    "                           plot_title=plot_title,\n",
    "                           color_bar=True, \n",
    "                           color_bar_title=color_bar_title,\n",
    "                           symbol_size=symbol_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[293], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m     tooltips\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNightlights (mean)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@nightlights_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     42\u001b[0m reversed_palette \u001b[38;5;241m=\u001b[39m Viridis256[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m color_mapper \u001b[38;5;241m=\u001b[39m LinearColorMapper(palette\u001b[38;5;241m=\u001b[39mreversed_palette, \n\u001b[0;32m---> 44\u001b[0m                                   low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean_wealth_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     45\u001b[0m                                   high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(source\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_wealth_index\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     47\u001b[0m color_spec \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_wealth_index\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m: color_mapper\n\u001b[1;32m     50\u001b[0m }\n\u001b[1;32m     53\u001b[0m plot_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Mean Wealth Index\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "if CREATE_DHS_MAPS:\n",
    "\n",
    "    # Initialize the data dictionary with mandatory fields\n",
    "    data_dict = {\n",
    "        'cluster_id': geospatial_df.index,\n",
    "        'lat': geospatial_df['lat'],\n",
    "        'lon': geospatial_df['lon'],\n",
    "        'mean_wealth_index': geospatial_df['mean_wealth_index'],\n",
    "        'mercator_x': mercator_x,\n",
    "        'mercator_y': mercator_y\n",
    "    }\n",
    "\n",
    "    # Add optional fields if they exist in the dataframe\n",
    "    if 'rainfall_mean' in geospatial_df.columns:\n",
    "        data_dict['rainfall_mean'] = geospatial_df['rainfall_mean']\n",
    "    if 'population_mean' in geospatial_df.columns:\n",
    "        data_dict['population_mean'] = geospatial_df['population_mean']\n",
    "    if 'nightlights_mean' in geospatial_df.columns:\n",
    "        data_dict['nightlights_mean'] = geospatial_df['nightlights_mean']\n",
    "\n",
    "    # Create the ColumnDataSource with the dynamically constructed data_dict\n",
    "    source = ColumnDataSource(data=data_dict)\n",
    "\n",
    "    # Define mandatory tooltips with percentage format for Mean Wealth Index\n",
    "    tooltips = [\n",
    "        (\"Cluster ID\", \"@cluster_id\"),\n",
    "        (\"Latitude\", \"@lat\"),\n",
    "        (\"Longitude\", \"@lon\"),\n",
    "        (\"Mean Wealth Index\", \"@mean_wealth_index{0.0%}\")\n",
    "    ]\n",
    "\n",
    "    # Conditionally add optional tooltips based on available data\n",
    "    if 'rainfall_mean' in data_dict:\n",
    "        tooltips.append((\"Rainfall (mean)\", \"@rainfall_mean\"))\n",
    "    if 'population_mean' in data_dict:\n",
    "        tooltips.append((\"Population (mean)\", \"@population_mean\"))\n",
    "    if 'nightlights_mean' in data_dict:\n",
    "        tooltips.append((\"Nightlights (mean)\", \"@nightlights_mean\"))\n",
    "\n",
    "\n",
    "\n",
    "    reversed_palette = Viridis256[::-1]\n",
    "    color_mapper = LinearColorMapper(palette=reversed_palette, \n",
    "                                      low=min(source.data['mean_wealth_index']), \n",
    "                                      high=max(source.data['mean_wealth_index']))\n",
    "\n",
    "    color_spec = {\n",
    "        'field': 'mean_wealth_index', \n",
    "        'transform': color_mapper\n",
    "    }\n",
    "\n",
    "\n",
    "    plot_title = f'{country_code} Mean Wealth Index'\n",
    "\n",
    "    color_bar_title = \"Mean Wealth Index\"\n",
    "\n",
    "    case = f'{country_code}_Mean_Wealth_Index'\n",
    "\n",
    "    out_dir = results_config.PLOT_GEOSPATIAL_DIR\n",
    "\n",
    "    create_geospatial_plot(source, \n",
    "                           tooltips, \n",
    "                           color_spec, \n",
    "                           out_dir, \n",
    "                           case=case,\n",
    "                           plot_title=plot_title,\n",
    "                           color_bar=True, \n",
    "                           color_bar_title=color_bar_title,\n",
    "                           symbol_size=symbol_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Vaccination rates data is empty.\n",
      "WARNING: Fresh water data is empty.\n",
      "WARNING: Wealth index data is empty.\n",
      "WARNING: Electricity data is empty.\n",
      "WARNING: Radio data is empty.\n",
      "WARNING: TV data is empty.\n",
      "Skipping plot for JO: Variogram of Vaccination Rates, no valid data.\n",
      "Skipping plot for JO: Variogram of Electricity Coverage, no valid data.\n",
      "Skipping plot for JO: Variogram of Fresh Water Access, no valid data.\n",
      "Skipping plot for JO: Variogram of Wealth Index, no valid data.\n",
      "Skipping plot for JO: Variogram of Radio, no valid data.\n",
      "Skipping plot for JO: Variogram of TV, no valid data.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[294], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m out_dir\u001b[38;5;241m=\u001b[39mresults_config\u001b[38;5;241m.\u001b[39mPLOT_VARIOGRAM_DIR\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplot_variograms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgeospatial_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43maoi_configurations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvariogram_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExponential\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_lags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_lag_km\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/BMGF/Deep-Learning-Global-Health-Analytics/./GIS-Image-Stack-Processing/project_utils/plot_utils.py:650\u001b[0m, in \u001b[0;36mplot_variograms\u001b[0;34m(country_code, geospatial_df, aoi_configurations, out_dir, normalize, variogram_model, n_lags, max_lag_km)\u001b[0m\n\u001b[1;32m    647\u001b[0m images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mopen(buf) \u001b[38;5;28;01mfor\u001b[39;00m buf \u001b[38;5;129;01min\u001b[39;00m buffers]\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# Get dimensions of the images\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m widths, heights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(image\u001b[38;5;241m.\u001b[39msize \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images))\n\u001b[1;32m    652\u001b[0m \u001b[38;5;66;03m# Calculate the total height for stacking images\u001b[39;00m\n\u001b[1;32m    653\u001b[0m total_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(heights)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "out_dir=results_config.PLOT_VARIOGRAM_DIR\n",
    "\n",
    "plot_variograms(country_code, \n",
    "                geospatial_df, \n",
    "                aoi_configurations, \n",
    "                out_dir=out_dir,\n",
    "                normalize=False, \n",
    "                variogram_model='Exponential',\n",
    "                n_lags=50,\n",
    "                max_lag_km=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39-pt-test)",
   "language": "python",
   "name": "py39-pt-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
