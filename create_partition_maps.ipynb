{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition Map Creation\n",
    "\n",
    "This notebook uses the DHS cluster data to partion the clusters into train and validation segments.\n",
    "\n",
    "\n",
    "## File System Structure\n",
    "\n",
    "## Input\n",
    "\n",
    "DHS data is used as the basis for creating partition maps for each country based on the location of clusters. \n",
    "\n",
    "<pre style=\"font-family: monospace;\">\n",
    "./GIS-Image-Stack-Processing\n",
    "    /DHS\n",
    "        /County specific folders containing DHS files\n",
    "</pre>\n",
    "\n",
    "## Output\n",
    "<pre style=\"font-family: monospace;\">\n",
    "./GIS-Image-Stack-Processing\n",
    "    /AOI/\n",
    "        Partitions/\n",
    "            PK/\n",
    "                <span style=\"color: blue;\">PK_all.json</span> \n",
    "                <span style=\"color: blue;\">PK_train.json</span> \n",
    "                <span style=\"color: blue;\">PK_valid.json</span> \n",
    "            TD/\n",
    "                <span style=\"color: blue;\">TD_all.json</span> \n",
    "                <span style=\"color: blue;\">TD_train.json</span> \n",
    "                <span style=\"color: blue;\">TD_valid.json</span> \n",
    "</pre>\n",
    "\n",
    "\n",
    "## Required Configurations\n",
    "\n",
    "The following configuration is required for each execution of this notebook: the two-letter country code.\n",
    "\n",
    "<pre style=\"font-family: monospace;\">\n",
    "<span style=\"color: blue;\">country_code  = 'PK'</span>      # Set the country code\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# REQUIRED CONFIGURATIONS HERE\n",
    "#-------------------------------------------------\n",
    "country_code  = 'PK'      # Set the country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./GIS-Image-Stack-Processing')  # Adjust path if `gist_utils` is moved\n",
    "# Import module that contains several convenience functions (e.g., gdal wrappers)\n",
    "from gist_utils import *\n",
    "\n",
    "from gist_utils.aoi_configurations import aoi_configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIS_ROOT = './GIS-Image-Stack-Processing'\n",
    "PRT_ROOT = './GIS-Image-Stack-Processing/AOI/Partitions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_partition = os.path.join(PRT_ROOT, f'{country_code}', f'{country_code}_train.json')\n",
    "valid_partition = os.path.join(PRT_ROOT, f'{country_code}', f'{country_code}_valid.json')\n",
    "all_partition   = os.path.join(PRT_ROOT, f'{country_code}', f'{country_code}_all.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DHS Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = os.path.join(GIS_ROOT, aoi_configurations[country_code]['shapefile'])\n",
    "recode_hr_path = os.path.join(GIS_ROOT, aoi_configurations[country_code]['recode_hr'])\n",
    "recode_kr_path = os.path.join(GIS_ROOT, aoi_configurations[country_code]['recode_kr'])\n",
    "\n",
    "# DHS Column Headings\n",
    "dhs_cluster_field  = 'DHSCLUST'\n",
    "dhs_lat_field      = 'LATNUM'\n",
    "dhs_lon_field      = 'LONGNUM'\n",
    "\n",
    "# Map Heading to new names\n",
    "cluster_id   = 'cluster_id'\n",
    "cluster_lat  = 'lat'\n",
    "cluster_lon  = 'lon'\n",
    "\n",
    "# The following mappings are used to rename DHS column headings to more meaningful names\n",
    "cluster_column_mapping = {\n",
    "    dhs_cluster_field: cluster_id,\n",
    "    dhs_lat_field: cluster_lat,\n",
    "    dhs_lon_field: cluster_lon\n",
    "}\n",
    "\n",
    "# DHS Household recode column name mapping\n",
    "hr_column_mapping = {\n",
    "    'HV001': cluster_id,\n",
    "    'HV201': 'water_access',\n",
    "    'HV206': 'electricity_access',\n",
    "    'HV208': 'radio_access',\n",
    "    'HV209': 'television_access',\n",
    "    'HV270': 'wealth_index'\n",
    "}\n",
    "\n",
    "# DHS Child recode column name mapping\n",
    "kr_column_mapping = {\n",
    "    'V001': cluster_id,\n",
    "    'H7': 'dpt1',\n",
    "    'H8': 'dpt2',\n",
    "    'H9': 'dpt3'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract DHS Cluster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erroneous clusters detected and removed: [535]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billk/dev/BMGF/Prithvi-Fine-Tuned-Global-Health/./GIS-Image-Stack-Processing/gist_utils/gist_utils.py:682: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_data[cluster_field] =         cluster_data[cluster_field].astype(float).astype(int)\n"
     ]
    }
   ],
   "source": [
    "cluster_df, erroneous_cluster_ids = extract_cluster_data(shapefile_path, dhs_cluster_field, dhs_lat_field, dhs_lon_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[535]\n"
     ]
    }
   ],
   "source": [
    "print(erroneous_cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster_id        lat        lon\n",
      "0           1  36.449918  72.571558\n",
      "1           2  35.891914  71.726873\n",
      "2           3  35.169566  71.834458\n",
      "3           4  35.424729  72.163931\n",
      "4           5  35.005696  71.776478\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "# Use the mapping to select and rename columns\n",
    "cluster_df = cluster_df[list(cluster_column_mapping.keys())].rename(columns=cluster_column_mapping)\n",
    "\n",
    "print(cluster_df.head())\n",
    "print(cluster_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Partition Maps\n",
    "\n",
    "This function creates a partition map file that specifies which cluster IDs are to be used for the \n",
    "given partiion. An input longitude threshold is currently used to partition data between train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_partition_maps(cluster_df, cluster_id, cluster_lon, country_code, longitude_threshold, output_train='train.json', output_valid='valid.json', output_all='all.json'):\n",
    "    \n",
    "    # Extract the cluster IDs and longitudes from the DataFrame\n",
    "    cluster_ids = cluster_df[cluster_id].tolist()\n",
    "    longitudes = cluster_df[cluster_lon].tolist()\n",
    "\n",
    "    # Initialize dictionaries for training, validation, and all partition maps\n",
    "    train_partition = []\n",
    "    valid_partition = []\n",
    "    all_partition = cluster_ids  # This will include all cluster IDs\n",
    "\n",
    "    # Assign cluster IDs to the appropriate partition based on the longitude threshold\n",
    "    for cid, lon in zip(cluster_ids, longitudes):\n",
    "        if lon > longitude_threshold:\n",
    "            train_partition.append(cid)\n",
    "        else:\n",
    "            valid_partition.append(cid)\n",
    "\n",
    "    # Prepare dictionary structures for JSON\n",
    "    train_partition_map = {f\"{country_code}\": train_partition}\n",
    "    valid_partition_map = {f\"{country_code}\": valid_partition}\n",
    "    all_partition_map = {f\"{country_code}\": all_partition}\n",
    "\n",
    "    # Ensure directory exists before saving JSON files\n",
    "    for output_file in [output_train, output_valid, output_all]:\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the train partition map to a JSON file\n",
    "    with open(output_train, 'w') as f:\n",
    "        json.dump(train_partition_map, f, indent=4)\n",
    "    print(f\"Train partition map saved to: {output_train}\")\n",
    "\n",
    "    # Save the valid partition map to a JSON file\n",
    "    with open(output_valid, 'w') as f:\n",
    "        json.dump(valid_partition_map, f, indent=4)\n",
    "    print(f\"Valid partition map saved to: {output_valid}\")\n",
    "\n",
    "    # Save the all partition map to a JSON file\n",
    "    with open(output_all, 'w') as f:\n",
    "        json.dump(all_partition_map, f, indent=4)\n",
    "    print(f\"All partition map saved to:   {output_all}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train partition map saved to: ./GIS-Image-Stack-Processing/AOI/Partitions/PK/PK_train.json\n",
      "Valid partition map saved to: ./GIS-Image-Stack-Processing/AOI/Partitions/PK/PK_valid.json\n",
      "All partition map saved to:   ./GIS-Image-Stack-Processing/AOI/Partitions/PK/PK_all.json\n"
     ]
    }
   ],
   "source": [
    "# Use the CRS longitude for the AOI as a threshold for partitioning the clusters. This could be improved \n",
    "# to more appropriately partition the data, but serves the purpose to protyping the capability.\n",
    "longitude_threshold = aoi_configurations[country_code]['crs_lon']\n",
    "\n",
    "generate_partition_maps(cluster_df, \n",
    "                        cluster_id, \n",
    "                        cluster_lon, \n",
    "                        country_code, \n",
    "                        longitude_threshold, \n",
    "                        train_partition, \n",
    "                        valid_partition,\n",
    "                        all_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39-pt)",
   "language": "python",
   "name": "clone_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
